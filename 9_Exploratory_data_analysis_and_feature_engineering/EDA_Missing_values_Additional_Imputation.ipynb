{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Imputation Using Multivariate Imputation by Chained Equation (MICE)\n",
    "\n",
    "This type of imputation works by filling the missing data multiple times. Multiple Imputations (MIs) are much better than a \n",
    "single imputation as it measures the uncertainty of the missing values in a better way. The chained equations approach is also \n",
    "very flexible and can handle different variables of different data types (ie., continuous or binary) as well as complexities \n",
    "such as bounds or survey skip patterns. For more information on the algorithm mechanics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 5. Imputation Using Deep Learning (Datawig)\n",
    "\n",
    "This method works very well with categorical and non-numerical features. It is a library that learns Machine Learning models using Deep Neural Networks to impute missing values in a dataframe. \n",
    "It also supports both CPU and GPU for training.\n",
    "\n",
    "##### Pros:\n",
    "Quite accurate compared to other methods.\n",
    "It has some functions that can handle categorical data (Feature Encoder).\n",
    "It supports CPUs and GPUs.\n",
    "##### Cons:\n",
    "Single Column imputation.\n",
    "Can be quite slow with large datasets.\n",
    "You have to specify the columns that contain information about the target column that will be imputed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 6. Other Imputation Methods:\n",
    "\n",
    "###### a) Stochastic regression imputation:\n",
    "It is quite similar to regression imputation which tries to predict the missing values by regressing it from other related variables in the same dataset plus some random residual value.\n",
    "Extrapolation and Interpolation:\n",
    "It tries to estimate values from other observations within the range of a discrete set of known data points.\n",
    "###### b) Hot-Deck imputation:\n",
    "Works by randomly choosing the missing value from a set of related and similar variables.\n",
    "\n",
    "In conclusion, there is no perfect way to compensate for the missing values in a dataset. Each strategy can perform better for certain datasets and missing data types but may perform much worse on other types of datasets. There are some set rules to decide which strategy to use for particular types of missing values, but beyond that, you should experiment and check which model works best \n",
    "for your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source from : https://towardsdatascience.com/4-tips-for-advanced-feature-engineering-and-preprocessing-ec11575c09ea\n",
    "#### 1. Resampling Imbalanced Data\n",
    "\n",
    "In real world scenario, you will encounter imbalanced data more often than not (i.e: More number of records training/ test data). This does not necessarily have to be a problem if your target only has a slight imbalance. \n",
    "\n",
    "You could then resolve it by using proper validation measures for the data such as Balanced Accuracy, Precision-Recall Curves or F1-score. Unfortunately, this is not always the case and your target variable might be highly imbalanced (e.g., 10:1). \n",
    "\n",
    "Instead, you can oversample the minority target in order to introduce balance using a technique called SMOTE.\n",
    "\n",
    "##### SMOTE\n",
    "More information on paper: https://jair.org/index.php/jair/article/view/10302\n",
    "\n",
    "SMOTE stands for Synthetic Minority Oversampling Technique and is an oversampling technique used to increase the samples in a minority class.\n",
    "\n",
    "It generates new samples by looking at the feature space of the target and detecting nearest neighbors. Then, it simply selects similar samples and changes a column at a time randomly within the feature space of the neighboring samples.\n",
    "The module to implement SMOTE can be found within the imbalanced-learn package. You can simply import the package and apply a fit_transform:\n",
    "\n",
    "There are several strategies that you can take when oversampling using SMOTE:\n",
    "'minority': resample only the minority class;\n",
    "'not minority': resample all classes but the minority class;\n",
    "'not majority': resample all classes but the majority class;\n",
    "'all': resample all classes;\n",
    "\n",
    "When dict, the keys correspond to the targeted classes. The values correspond to the desired number of samples for each targeted class.\n",
    "I chose to use a dictionary to specify the extent to which I wanted to oversample my data.\n",
    "\n",
    "If you have categorical variables in your dataset SMOTE is likely to create values for those variables that cannot happen. For example, if you have a variable called is Male, which could only take 0 or 1, then SMOTE might create 0.365 as a value.\n",
    "Instead, you can use SMOTENC which takes into account the nature of categorical variables. This version is also available in the imbalanced-learn package. \n",
    "\n",
    "Another imporrtant thing to consider, Make sure to oversample after creating the train/test split so that you only oversample the train data. You typically do not want to test your model on synthetic data.\n",
    "\n",
    "##### 2. Creating New Features\n",
    "\n",
    "To improve the quality and predictive power of our models, new features from existing variables are often created. We can create some interaction (e.g., multiply or divide) between each pair of variables hoping to find an interesting new feature. This, however, is a lengthy process and requires a significant amount of coding. Fortunately, this can be automated using Deep Feature Synthesis.\n",
    "\n",
    "###### Deep Feature Synthesis\n",
    "\n",
    "Deep feature synthesis (DFS) is an algorithm which enables you to quickly create new variables with varying depth. For example, you can multiply pairs of columns but you can also choose to first multiply Column A with Column B and then add Column C.\n",
    "First, let me introduce the data I will be using for the example. I have chosen to use HR analytics data since the features are easily interpretable\n",
    "\n",
    "\n",
    "The first step is to create an entity from which relationships can be created with other tables if necessary. Next, we can simply run ft.dfs in order to create new variables. We specify how variables are created with the parameter trans_primitives. We chose to either add numeric variables together or multiply.\n",
    "\n",
    "DFS's one more interesting feature is that it can create new variables from aggregations between tables (e.g., facts and dimensions)\n",
    "\n",
    "Run ft.list_primitives()in order to see the full list of aggregation that you can do. It even handles timestamps, null values, and long/lat information.\n",
    "\n",
    "###### Handling Missing Values\n",
    "\n",
    "As always, there is no one best way of dealing with missing values. Depending on your data it might be sufficient to simply fill them with the mean or mode of certain groups. However, there are advanced techniques that use known parts of the data to impute the missing values.\n",
    "One such method is called IterativeImputer a new package in Scikit-Learn which is based on the popular R algorithm for imputing missing variables, MICE.\n",
    "\n",
    "\n",
    "###### Iterative Imputer\n",
    "The Iterative Imputer is developed by Scikit-Learn and models each feature with missing values as a function of other features. It uses that as an estimate for imputation. At each step, a feature is selected as output y and all other features are treated as inputs X. A regressor is then fitted on X and y and used to predict the missing values of y. This is done for each feature and repeated for several imputation rounds.\n",
    "\n",
    "The great thing about this method is that it allows you to use an estimator of your choosing. I used a RandomForestRegressor to mimic the behavior of the frequently used missForest in R.\n",
    "\n",
    "If you have sufficient data, then it might be an attractive option to simply delete samples with missing data. However, keep in mind that it could create bias in your data. Perhaps the missing data follows a pattern that you miss out on.\n",
    "\n",
    "The Iterative Imputer allows for different estimators to be used. After some testing, I found out that you can even use Catboost as an estimator! Unfortunately, LightGBM and XGBoost do not work since their random state names differ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
