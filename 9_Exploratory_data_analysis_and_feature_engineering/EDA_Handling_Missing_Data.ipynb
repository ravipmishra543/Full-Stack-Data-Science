{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are three different types of missing data\n",
    "\n",
    "1) Missing completely at random (MCAR)\n",
    "2) Missing at random (MAR)\n",
    "3) Not missing at random (NMAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### popular ways for data imputation for cross-sectional datasets\n",
    "Source:\n",
    "    \n",
    "https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779\n",
    "    \n",
    "    \n",
    "####  1. Do Nothing:\n",
    "\n",
    "We can just let the algorithm handle the missing data. Some algorithms can factor in the missing values and learn the \n",
    "best imputation values for the missing data based on the training loss reduction (ie. XGBoost). \n",
    "Some others have the option to just ignore them (ie. LightGBM — use_missing=false). However, other algorithms will panic and \n",
    "throw an error complaining about the missing values (ie. Scikit learn — LinearRegression). \n",
    "In that case, you will need to handle the missing data and clean it before feeding it to the algorithm.\n",
    "\n",
    "#### 2. Imputation Using (Mean/Median) Values:\n",
    "\n",
    "This works by calculating the mean/median of the non-missing values in a column and then replacing the missing values within \n",
    "each column separately and independently from the others. It can only be used with numeric data.\n",
    "\n",
    "##### Pros:\n",
    "Easy and fast.\n",
    "Works well with small numerical datasets.\n",
    "\n",
    "##### Cons:\n",
    "Doesn’t factor the correlations between features. It only works on the column level.\n",
    "Will give poor results on encoded categorical features (do NOT use it on categorical features).\n",
    "Not very accurate.\n",
    "Doesn’t account for the uncertainty in the imputations.\n",
    "    \n",
    "##### 3. Imputation Using (Most Frequent) or (Zero/Constant) Values:\n",
    "Most Frequent is another statistical strategy to impute missing values and YES!! It works with categorical features \n",
    "(strings or numerical representations) by replacing missing data with the most frequent values within each column.\n",
    "    \n",
    "##### Pros:\n",
    "Works well with categorical features.\n",
    "\n",
    "##### Cons:\n",
    "It also doesn’t factor the correlations between features.\n",
    "It can introduce bias in the data.\n",
    "\n",
    "Zero or Constant imputation — as the name suggests — it replaces the missing values with either zero or any constant \n",
    "value you specify\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputation Using k-NN:\n",
    "    \n",
    "The algorithm uses ‘feature similarity’ to predict the values of any new data points. This means that the new point is assigned \n",
    "a value based on how closely it resembles the points in the training set. This can be very useful in making predictions about \n",
    "the missing values by finding the k’s closest neighbours to the observation with missing data and then imputing them based \n",
    "on the non-missing values in the neighbourhood. \n",
    "\n",
    "It creates a basic mean impute then uses the resulting complete list to construct a KDTree. Then, it uses the resulting KDTree \n",
    "to compute nearest neighbours (NN). After it finds the k-NNs, it takes the weighted average of them\n",
    "\n",
    "##### Pros:\n",
    "Can be much more accurate than the mean, median or most frequent imputation methods (It depends on the dataset).\n",
    "##### Cons:\n",
    "Computationally expensive. KNN works by storing the whole training dataset in memory.\n",
    "K-NN is quite sensitive to outliers in the data (unlike SVM)\n",
    "\n",
    "Let's discuss KNN Imputer with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\10635638\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\10635638\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\10635638\\anaconda3\\lib\\site-packages (from scikit-learn) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\10635638\\anaconda3\\lib\\site-packages (from scikit-learn) (1.22.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\10635638\\anaconda3\\lib\\site-packages (from scikit-learn) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### How does KNN Imputer work?\n",
    "According scikit-learn docs: Each sample’s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close. By default, a euclidean distance metric that supports missing values, nan_euclidean_distances, is used to find the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>second</th>\n",
       "      <th>Third</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   first  second  Third\n",
       "0  112.0    30.0    NaN\n",
       "1   90.0    45.0   40.0\n",
       "2    NaN    56.0   80.0\n",
       "3   89.0     NaN   98.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Dataframe with Missing Values\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df= {'first': [112, 90, np.nan, 89],\n",
    "    'second': [30,45,56, np.nan],\n",
    "    'Third':[np.nan, 40, 80, 98]}\n",
    "\n",
    "df= pd.DataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[112. ,  30. ,  69. ],\n",
       "       [ 90. ,  45. ,  40. ],\n",
       "       [100.5,  56. ,  80. ],\n",
       "       [ 89. ,  43. ,  98. ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 Initialize KNNImputer\n",
    "# You can define your own n_neighbors value (as its typical of KNN algorithm)\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "df_filled = imputer.fit_transform(df)\n",
    "df_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Imputation Using Multivariate Imputation by Chained Equation (MICE)\n",
    "\n",
    "This type of imputation works by filling the missing data multiple times. Multiple Imputations (MIs) are much better than a \n",
    "single imputation as it measures the uncertainty of the missing values in a better way. The chained equations approach is also \n",
    "very flexible and can handle different variables of different data types (ie., continuous or binary) as well as complexities \n",
    "such as bounds or survey skip patterns. For more information on the algorithm mechanics.\n",
    "\n",
    "\n",
    "##### 5. Imputation Using Deep Learning (Datawig)\n",
    "\n",
    "This method works very well with categorical and non-numerical features. It is a library that learns Machine Learning models using Deep Neural Networks to impute missing values in a dataframe. \n",
    "It also supports both CPU and GPU for training.\n",
    "\n",
    "##### Pros:\n",
    "Quite accurate compared to other methods.\n",
    "It has some functions that can handle categorical data (Feature Encoder).\n",
    "It supports CPUs and GPUs.\n",
    "##### Cons:\n",
    "Single Column imputation.\n",
    "Can be quite slow with large datasets.\n",
    "You have to specify the columns that contain information about the target column that will be imputed.\n",
    "\n",
    "###### 6. Other Imputation Methods:\n",
    "\n",
    "###### a) Stochastic regression imputation:\n",
    "It is quite similar to regression imputation which tries to predict the missing values by regressing it from other related variables in the same dataset plus some random residual value.\n",
    "Extrapolation and Interpolation:\n",
    "It tries to estimate values from other observations within the range of a discrete set of known data points.\n",
    "###### b) Hot-Deck imputation:\n",
    "Works by randomly choosing the missing value from a set of related and similar variables.\n",
    "\n",
    "In conclusion, there is no perfect way to compensate for the missing values in a dataset. Each strategy can perform better for certain datasets and missing data types but may perform much worse on other types of datasets. There are some set rules to decide which strategy to use for particular types of missing values, but beyond that, you should experiment and check which model works best \n",
    "for your dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source from : https://towardsdatascience.com/4-tips-for-advanced-feature-engineering-and-preprocessing-ec11575c09ea\n",
    "#### 1. Resampling Imbalanced Data\n",
    "\n",
    "In real world scenario, you will encounter imbalanced data more often than not (i.e: More number of records training/ test data). This does not necessarily have to be a problem if your target only has a slight imbalance. \n",
    "\n",
    "You could then resolve it by using proper validation measures for the data such as Balanced Accuracy, Precision-Recall Curves or F1-score. Unfortunately, this is not always the case and your target variable might be highly imbalanced (e.g., 10:1). \n",
    "\n",
    "Instead, you can oversample the minority target in order to introduce balance using a technique called SMOTE.\n",
    "\n",
    "##### SMOTE\n",
    "More information on paper: https://jair.org/index.php/jair/article/view/10302\n",
    "\n",
    "SMOTE stands for Synthetic Minority Oversampling Technique and is an oversampling technique used to increase the samples in a minority class.\n",
    "\n",
    "It generates new samples by looking at the feature space of the target and detecting nearest neighbors. Then, it simply selects similar samples and changes a column at a time randomly within the feature space of the neighboring samples.\n",
    "The module to implement SMOTE can be found within the imbalanced-learn package. You can simply import the package and apply a fit_transform:\n",
    "\n",
    "There are several strategies that you can take when oversampling using SMOTE:\n",
    "'minority': resample only the minority class;\n",
    "'not minority': resample all classes but the minority class;\n",
    "'not majority': resample all classes but the majority class;\n",
    "'all': resample all classes;\n",
    "\n",
    "When dict, the keys correspond to the targeted classes. The values correspond to the desired number of samples for each targeted class.\n",
    "I chose to use a dictionary to specify the extent to which I wanted to oversample my data.\n",
    "\n",
    "If you have categorical variables in your dataset SMOTE is likely to create values for those variables that cannot happen. For example, if you have a variable called is Male, which could only take 0 or 1, then SMOTE might create 0.365 as a value.\n",
    "Instead, you can use SMOTENC which takes into account the nature of categorical variables. This version is also available in the imbalanced-learn package. \n",
    "\n",
    "Another imporrtant thing to consider, Make sure to oversample after creating the train/test split so that you only oversample the train data. You typically do not want to test your model on synthetic data.\n",
    "\n",
    "##### 2. Creating New Features\n",
    "\n",
    "To improve the quality and predictive power of our models, new features from existing variables are often created. We can create some interaction (e.g., multiply or divide) between each pair of variables hoping to find an interesting new feature. This, however, is a lengthy process and requires a significant amount of coding. Fortunately, this can be automated using Deep Feature Synthesis.\n",
    "\n",
    "###### Deep Feature Synthesis\n",
    "\n",
    "Deep feature synthesis (DFS) is an algorithm which enables you to quickly create new variables with varying depth. For example, you can multiply pairs of columns but you can also choose to first multiply Column A with Column B and then add Column C.\n",
    "First, let me introduce the data I will be using for the example. I have chosen to use HR analytics data since the features are easily interpretable\n",
    "\n",
    "\n",
    "The first step is to create an entity from which relationships can be created with other tables if necessary. Next, we can simply run ft.dfs in order to create new variables. We specify how variables are created with the parameter trans_primitives. We chose to either add numeric variables together or multiply.\n",
    "\n",
    "DFS's one more interesting feature is that it can create new variables from aggregations between tables (e.g., facts and dimensions)\n",
    "\n",
    "Run ft.list_primitives()in order to see the full list of aggregation that you can do. It even handles timestamps, null values, and long/lat information.\n",
    "\n",
    "###### Handling Missing Values\n",
    "\n",
    "As always, there is no one best way of dealing with missing values. Depending on your data it might be sufficient to simply fill them with the mean or mode of certain groups. However, there are advanced techniques that use known parts of the data to impute the missing values.\n",
    "One such method is called IterativeImputer a new package in Scikit-Learn which is based on the popular R algorithm for imputing missing variables, MICE.\n",
    "\n",
    "\n",
    "###### Iterative Imputer\n",
    "The Iterative Imputer is developed by Scikit-Learn and models each feature with missing values as a function of other features. It uses that as an estimate for imputation. At each step, a feature is selected as output y and all other features are treated as inputs X. A regressor is then fitted on X and y and used to predict the missing values of y. This is done for each feature and repeated for several imputation rounds.\n",
    "\n",
    "The great thing about this method is that it allows you to use an estimator of your choosing. I used a RandomForestRegressor to mimic the behavior of the frequently used missForest in R.\n",
    "\n",
    "If you have sufficient data, then it might be an attractive option to simply delete samples with missing data. However, keep in mind that it could create bias in your data. Perhaps the missing data follows a pattern that you miss out on.\n",
    "\n",
    "The Iterative Imputer allows for different estimators to be used. After some testing, I found out that you can even use Catboost as an estimator! Unfortunately, LightGBM and XGBoost do not work since their random state names differ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
